{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "hcXFtCPkDqoxd6Hwrjx1ME",
     "type": "MD"
    }
   },
   "source": [
    "# **This is the collaborative code for the group project QTEM Data Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "RBcxvzmNL2UKbfeFkW7aV7",
     "type": "MD"
    }
   },
   "source": [
    "###  Before editing or contributing to this code, you should create a side notebook where you can test your ideas\n",
    "You can do so by cloning this notebook under another name\n",
    "\n",
    "Also, when adding a cell, make sure you comment and put a title on what you are doing so we can keep track of all steps\n",
    "\n",
    "Have FUN!!!\n",
    "\n",
    "## Markdowns: \n",
    "\n",
    "### You can change the cell type to Markdown in the dropdown list in the top menu\n",
    "### Add a cell, then select Mardown\n",
    "### You can then write text under different format\n",
    "### Use the \"#\" symbol before your text to make it bigger\n",
    "# \"#\" will make it Super large\n",
    "## \"##\" a bit smaller\n",
    "### \"###\" a bit smaller\n",
    "##### \"####\" you get the point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____ __________________ _________ _______ __________ ______________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Hy5AE4C3yU1sHWUkVI4sTk",
     "type": "MD"
    }
   },
   "source": [
    "## 1. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "l0VMpiYvVHyQGAl5HB81ey",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "vxjFia3zrHzPmN8t1uzcnR",
     "type": "MD"
    }
   },
   "source": [
    "## 2. Importing data and defining datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important information / clarification about the datasets: \n",
    "\n",
    "SENT BY MAIO VITTORIO THROUGH FORUM: \n",
    "\n",
    "\"\"Cartier Data Science expert would like to make a clarification on the transactions in the dataset: in the dataset there are not only repurchasing clients coming back after more than 4 years, But also clients repurchasing clients coming back before this threshold.\n",
    "\n",
    "In terms of the project requirement, this does not impact what is expected from you. The statistics about repurchasing customers after 4 years is something that Cartier started to investigate and which would be the best starting point for your research. Of course, if it is possible to also investigate other types of returning customers, it could add a different perspective (and possibly a better insight) of repurchasing customers’ behaviour.\"\" \n",
    "\n",
    "#### This means we could / should also analyze the data from customers who return after less than 4 years\n",
    "#### I think we should start by the ones coming after 4 years since this is the central point \n",
    "#### We could dig deeper if we deem it necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "nyn7xcYNXoOQngwGrnjdPo",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_Calls = pd.read_csv(\"b. CARTIER_CALLS.csv\")\n",
    "df_Clienteling = pd.read_csv(\"c. CARTIER_CLIENTELING.csv\")\n",
    "df_Livechat = pd.read_csv(\"d. CARTIER_LIVECHAT.csv\")\n",
    "df_PrevSales = pd.read_csv(\"e. CARTIER_PREVIOUS_SALES.csv\")\n",
    "df_Sales = pd.read_csv(\"f. CARTIER_SALES.csv\")\n",
    "df_Wishlist = pd.read_csv(\"g. CARTIER_WISHLIST.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged sales and previous sales dataset with extra column [in_salesdataset] to indicate to which original dataset \n",
    "# the data belong\n",
    "\n",
    "df_Sales['in_salesdataset']=1\n",
    "df_PrevSales['in_salesdataset']=0\n",
    "\n",
    "# Rename column from Sale for the concat of datasets\n",
    "\n",
    "df_Sales.rename(columns = {'articleA':'ArticleA'},inplace = True)\n",
    "\n",
    "df_AllSales = pd.concat([df_Sales, df_PrevSales])\n",
    "df_AllSales = df_AllSales.sort_values(by='ClientID', ascending= True)\n",
    "\n",
    "#to export a dataframe to excel (for siqi): \n",
    "\n",
    "#df_BLABLABLA.to_csv('df_BLABLABLA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming and uniformizing some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['InvoiceHeader', 'Channel', 'TransactionDate', 'TransactionDate_FYYYY', 'TransactionCategory', 'ClientID', 'AgeAtTransaction', 'Gender', 'PersonBirthDate', 'WeddingDate', 'SpokenLanguage', 'WrittenLanguage', 'FirstSalesDate', 'FirstTransactionDate', 'ProductCategory', 'ProductSubCategory', 'ProductFunction', 'Turnover', 'quantity', 'seq_sales_trs', 'nb_days_since_last_sale', 'PurchasedMarketA', 'PurchasedRegionA', 'ResidencyRegionA', 'ResidencyMarketA', 'BoutiqueNameA', 'ResidencyCountryA', 'nationalityA', 'ArticleA', 'Hier_Lev_3_txtA', 'Hier_Lev_4_txtA', 'Hier_Lev_5_txtA', 'ProductCollectionA', 'in_salesdataset', 'NationalityA'] \n",
      "\n",
      " ['CreationDate', 'Marketcall', 'EscalatedTransferedFlag', 'CallID', 'Phone_Line_Interaction', 'InteractionType', 'AnsweredCallFlag', 'FlashCallsFlag', 'AbandonedCallsFlag', 'CommunicationTime', 'WaitingTime', 'ClientID', 'Motive', 'SubMotive'] \n",
      "\n",
      " ['ActivityId', 'ActivityStatus', 'ActivityCategory', 'ActivityType', 'Occasiontype', 'ClientID', 'StartDate', 'EndDate', 'DueDate', 'BoutiqueName', 'BoutiqueCountryA'] \n",
      "\n",
      " ['CreationDate', 'CaseId', 'ChatDuration', 'NumberofAgentMessage', 'NumberofClientMessage', 'ClientID', 'motive', 'submotive'] \n",
      "\n",
      " ['ClientID', 'wishlistID', 'articleA', 'createdDate', 'LastmodifiedDate', 'ProductCategory', 'ProductSubCategory', 'ProductCollectionA', 'ProductFunction', 'Hier_Lev_3_txtA', 'Hier_Lev_4_txtA', 'Hier_Lev_5_txtA']\n"
     ]
    }
   ],
   "source": [
    "# First let's compare all our column names\n",
    "datasets = [df_Calls,\n",
    "df_Clienteling,\n",
    "df_Livechat,\n",
    "df_Wishlist,]\n",
    "\n",
    "# Renaming\n",
    "\n",
    "df_Clienteling.rename(columns = {'clientID':'ClientID'}, inplace = True)\n",
    "df_Livechat.rename(columns = {'cLientid':'ClientID'}, inplace = True)\n",
    "df_Wishlist.rename(columns = {'clientID':'ClientID'}, inplace = True)\n",
    "\n",
    "\n",
    "print(list(df_AllSales),\"\\n\\n\", list(df_Calls),\"\\n\\n\", list(df_Clienteling),\"\\n\\n\", list(df_Livechat),\"\\n\\n\", list(df_Wishlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  InvoiceHeader   Channel TransactionDate  \\\n",
      "759803  000000201875164-GEMINI-/BIC/AZRIRBAJP00       Web      2020-06-12   \n",
      "568787  000000061204336-GEMINI-/BIC/AZRIRBAJP00  Boutique      2019-10-14   \n",
      "\n",
      "        TransactionDate_FYYYY TransactionCategory            ClientID  \\\n",
      "759803                   2021                Sale  0011i00000VhfANAAZ   \n",
      "568787                   2020              Repair  0011i00000VvE1VAAV   \n",
      "\n",
      "        AgeAtTransaction  Gender PersonBirthDate WeddingDate  ...  \\\n",
      "759803              50.0    Male      1971-08-19         NaN  ...   \n",
      "568787               NaN  Female             NaN         NaN  ...   \n",
      "\n",
      "       BoutiqueNameA ResidencyCountryA nationalityA  ArticleA Hier_Lev_3_txtA  \\\n",
      "759803           NaN              56.0          NaN     556.0            84.0   \n",
      "568787          76.0              56.0          NaN  123868.0             5.0   \n",
      "\n",
      "       Hier_Lev_4_txtA Hier_Lev_5_txtA  ProductCollectionA  in_salesdataset  \\\n",
      "759803             NaN             NaN                78.0                0   \n",
      "568787           157.0           539.0                57.0                0   \n",
      "\n",
      "        NationalityA  \n",
      "759803           NaN  \n",
      "568787          99.0  \n",
      "\n",
      "[2 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a sample dataset to work with for speed purposes if necessary (3% of size)\n",
    "\n",
    "#df_AllSalesSample = df_AllSales.head(int(round((len(df_allsales)/(100)),0)))\n",
    "# How to get a random sample of the dataset ??\n",
    "df_AllSalesSample=df_AllSales.sample(frac=0.03)\n",
    "print(df_AllSalesSample.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceHeader',\n",
       " 'Channel',\n",
       " 'TransactionDate',\n",
       " 'TransactionDate_FYYYY',\n",
       " 'TransactionCategory',\n",
       " 'ClientID',\n",
       " 'AgeAtTransaction',\n",
       " 'Gender',\n",
       " 'PersonBirthDate',\n",
       " 'WeddingDate',\n",
       " 'SpokenLanguage',\n",
       " 'WrittenLanguage',\n",
       " 'FirstSalesDate',\n",
       " 'FirstTransactionDate',\n",
       " 'ProductCategory',\n",
       " 'ProductSubCategory',\n",
       " 'ProductFunction',\n",
       " 'Turnover',\n",
       " 'quantity',\n",
       " 'seq_sales_trs',\n",
       " 'nb_days_since_last_sale',\n",
       " 'PurchasedMarketA',\n",
       " 'PurchasedRegionA',\n",
       " 'ResidencyRegionA',\n",
       " 'ResidencyMarketA',\n",
       " 'BoutiqueNameA',\n",
       " 'ResidencyCountryA',\n",
       " 'nationalityA',\n",
       " 'ArticleA',\n",
       " 'Hier_Lev_3_txtA',\n",
       " 'Hier_Lev_4_txtA',\n",
       " 'Hier_Lev_5_txtA',\n",
       " 'ProductCollectionA',\n",
       " 'in_salesdataset',\n",
       " 'NationalityA']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_AllSales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2169246"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df_AllSalesWithDuplicates = df_AllSales\n",
    "total_rows_before = df_AllSalesWithDuplicates.shape[0]\n",
    "total_rows_before\n",
    "\n",
    "# We have 2 173 083 rows in All_Sales dataset with duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "df_AllSales.drop_duplicates(inplace=True)\n",
    "\n",
    "total_rows_after = df_AllSales.shape[0]\n",
    "duplicates_count = total_rows_before - total_rows_after\n",
    "duplicates_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6a7ef9ce3db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Drop irrelevant columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_AllSales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Hier_Lev_3_txtA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Hier_Lev_4_txtA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Hier_Lev_5_txtA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m                 \u001b[1;31m# Check if label doesn't exist along axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36misin\u001b[1;34m(self, values, level)\u001b[0m\n\u001b[0;32m   4917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4918\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_index_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4919\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_string_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_lhs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_rhs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36misin\u001b[1;34m(comps, values)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;31m# If the the values include nan we need to check for nan explicitly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# since np.nan it not equal to np.nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "\n",
    "df_AllSales.drop([\"Hier_Lev_3_txtA\", \"Hier_Lev_4_txtA\", \"Hier_Lev_5_txtA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "\n",
    "df_AllSales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dUPRadFx3IO8tnqSVXv2MQ",
     "type": "MD"
    }
   },
   "source": [
    "## 3. Quick look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "I6Ly9SvBZdU7pyUvUZWWh9",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# To see the columns we have \n",
    "\n",
    "# Run this line if you wish to be able to visualize all the columns whenever you print out the dataframe\n",
    "\n",
    "#pd.set_option('max_columns', None)\n",
    "\n",
    "print(list(df_AllSales))\n",
    "#print(list(anything else you want to see))\n",
    "\n",
    "df_AllSales.describe()\n",
    "df_AllSales.info()\n",
    "AllSales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "MRlVkGJQ5qogO55sBV5NCR",
     "type": "MD"
    }
   },
   "source": [
    "## 4. Generating the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "7EUPdfz7NnQkQo0TdeDHZ8",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "### We can generate a profile to obtain description of our datasets\n",
    "### This can take some time if running report on large datasets\n",
    "### We can run the report on our sample dataset\n",
    "\n",
    "# Profile for Sales Sample\n",
    "\n",
    "#Profile_AllSalesSample = ProfileReport(df_Allsales, title=\"Pandas Profiling Report Sales\")\n",
    "\n",
    "# Visualizing the report\n",
    "\n",
    "#Report to iframe (will open report in notebook)\n",
    "\n",
    "#Profile_AllSalesSample.to_notebook_iframe()\n",
    "\n",
    "## Report to HTML (Will open up a new tab)\n",
    "#Profile_AllSalesSample.to_file(output_file='All Sales Sample report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next we would like to create new columns in [Allsales] to identify wether clientID is present in [Calls], [LiveChat], [Clienteling] or any one of the 3 (we can call this column: \"interaction' and assign 1 if there was interaction of any kind and 0 otherwise.\n",
    "\n",
    "### This will be needed to filter our data and run a multidimentional analysis\n",
    "#### To answer questions regarding contacts and communication between Cartier and Clients, we only need data concerning ClientsID who've had interactions with Cartier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [0 1] [0 1] [0 1]\n",
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# #Defining =1 when ClientID is present in the corresponding datasets\n",
    "df_AllSales = df_AllSales.assign(Calls=df_AllSales.ClientID.isin(df_Calls.ClientID).astype(int))\n",
    "df_AllSales = df_AllSales.assign(Clienteling=df_AllSales.ClientID.isin(df_Clienteling.ClientID).astype(int))\n",
    "df_AllSales = df_AllSales.assign(Livechat=df_AllSales.ClientID.isin(df_Livechat.ClientID).astype(int))\n",
    "df_AllSales = df_AllSales.assign(Wishlist=df_AllSales.ClientID.isin(df_Wishlist.ClientID).astype(int))\n",
    "\n",
    "\n",
    "print(df_AllSales.Calls.unique(),\n",
    "      df_AllSales.Clienteling.unique(),\n",
    "      df_AllSales.Livechat.unique(),\n",
    "      df_AllSales.Wishlist.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  InvoiceHeader   Channel TransactionDate  \\\n",
      "122626                                      NaN  Boutique      2019-12-14   \n",
      "134321                                      NaN  Boutique      2019-12-14   \n",
      "9874    000000010730476-GEMINI-/BIC/AZRIRBARP00  Boutique      2011-06-11   \n",
      "125277                                      NaN  Boutique      2019-12-14   \n",
      "22358   302109220925007-GEMINI-/BIC/AZRIRBARP00  Boutique      2022-09-25   \n",
      "4845    000000010959126-GEMINI-/BIC/AZRIRBARP00  Boutique      2013-04-08   \n",
      "110727                                      NaN  Boutique      2021-05-11   \n",
      "109418                                      NaN  Boutique      2019-02-12   \n",
      "13262                                       NaN  Boutique      2019-08-01   \n",
      "117890  240113000003532-GEMINI-/BIC/AZRIRBAPE00  Boutique      2019-07-02   \n",
      "16553   240303000001309-GEMINI-/BIC/AZRIRBAPE00  Boutique      2019-06-24   \n",
      "18388                                       NaN  Boutique      2021-05-11   \n",
      "409                                         NaN  Boutique      2019-05-08   \n",
      "20910                                       NaN  Boutique      2020-02-10   \n",
      "7646                                        NaN  Boutique      2016-12-10   \n",
      "4039    240113000003530-GEMINI-/BIC/AZRIRBAPE00  Boutique      2019-07-01   \n",
      "20155                                       NaN  Boutique      2021-06-15   \n",
      "123825  240303000001309-GEMINI-/BIC/AZRIRBAPE00  Boutique      2019-06-24   \n",
      "60779   120214000000854-GEMINI-/BIC/AZRIRBAPE00  Boutique      2022-05-18   \n",
      "113561                                      NaN  Boutique      2022-06-16   \n",
      "112295                                      NaN  Boutique      2022-06-16   \n",
      "129793                                      NaN  Boutique      2020-01-08   \n",
      "444964  122399000006415-GEMINI-/BIC/AZRIRBAPE00  Boutique      2022-09-10   \n",
      "9687                                        NaN  Boutique      2022-02-21   \n",
      "120224                                      NaN  Boutique      2021-07-03   \n",
      "125788                                      NaN  Boutique      2020-10-29   \n",
      "131471                                      NaN  Boutique      2020-07-26   \n",
      "130122                                      NaN  Boutique      2020-01-12   \n",
      "123192                                      NaN  Boutique      2021-07-03   \n",
      "19664   240109000002597-GEMINI-/BIC/AZRIRBAPE00  Boutique      2015-09-26   \n",
      "\n",
      "        TransactionDate_FYYYY TransactionCategory            ClientID  \\\n",
      "122626                   2020                Sale  0011i00000UNT9LAAX   \n",
      "134321                   2020                Sale  0011i00000UNT9LAAX   \n",
      "9874                     2012              Repair  0011i00000UNT9LAAX   \n",
      "125277                   2020                Sale  0011i00000UNT9LAAX   \n",
      "22358                    2023                Sale  0011i00000UNT9LAAX   \n",
      "4845                     2014              Repair  0011i00000UNT9LAAX   \n",
      "110727                   2022                Sale  0011i00000UNTHbAAP   \n",
      "109418                   2019                Sale  0011i00000UNTHbAAP   \n",
      "13262                    2020                Sale  0011i00000UNTHbAAP   \n",
      "117890                   2020                Sale  0011i00000UNTHbAAP   \n",
      "16553                    2020                Sale  0011i00000UNTHbAAP   \n",
      "18388                    2022                Sale  0011i00000UNTHbAAP   \n",
      "409                      2020                Sale  0011i00000UNTHbAAP   \n",
      "20910                    2020                Sale  0011i00000UNTHbAAP   \n",
      "7646                     2017                Sale  0011i00000UNTHbAAP   \n",
      "4039                     2020                Sale  0011i00000UNTHbAAP   \n",
      "20155                    2022                Sale  0011i00000UNTHbAAP   \n",
      "123825                   2020                Sale  0011i00000UNTHbAAP   \n",
      "60779                    2023                Sale  0011i00000UNTHbAAP   \n",
      "113561                   2023                Sale  0011i00000UNTM0AAP   \n",
      "112295                   2023                Sale  0011i00000UNTM0AAP   \n",
      "129793                   2020                Sale  0011i00000UNTM0AAP   \n",
      "444964                   2023                Sale  0011i00000UNTM0AAP   \n",
      "9687                     2022                Sale  0011i00000UNTM0AAP   \n",
      "120224                   2022                Sale  0011i00000UNTMyAAP   \n",
      "125788                   2021                Sale  0011i00000UNTMyAAP   \n",
      "131471                   2021                Sale  0011i00000UNTMyAAP   \n",
      "130122                   2020                Sale  0011i00000UNTMyAAP   \n",
      "123192                   2022                Sale  0011i00000UNTMyAAP   \n",
      "19664                    2016                Sale  0011i00000UNTMyAAP   \n",
      "\n",
      "        AgeAtTransaction Gender PersonBirthDate WeddingDate  ...  \\\n",
      "122626              56.0   Male      1964-10-13         NaN  ...   \n",
      "134321              56.0   Male      1964-10-13         NaN  ...   \n",
      "9874                48.0   Male      1964-10-13         NaN  ...   \n",
      "125277              56.0   Male      1964-10-13         NaN  ...   \n",
      "22358               59.0   Male      1964-10-13         NaN  ...   \n",
      "4845                50.0   Male      1964-10-13         NaN  ...   \n",
      "110727              28.0   Male      1994-11-23  2019-06-15  ...   \n",
      "109418              25.0   Male      1994-11-23  2019-06-15  ...   \n",
      "13262               26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "117890              26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "16553               26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "18388               28.0   Male      1994-11-23  2019-06-15  ...   \n",
      "409                 26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "20910               26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "7646                23.0   Male      1994-11-23  2019-06-15  ...   \n",
      "4039                26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "20155               28.0   Male      1994-11-23  2019-06-15  ...   \n",
      "123825              26.0   Male      1994-11-23  2019-06-15  ...   \n",
      "60779               29.0   Male      1994-11-23  2019-06-15  ...   \n",
      "113561              45.0   Male      1978-12-26  2004-06-22  ...   \n",
      "112295              45.0   Male      1978-12-26  2004-06-22  ...   \n",
      "129793              42.0   Male      1978-12-26  2004-06-22  ...   \n",
      "444964              45.0   Male      1978-12-26  2004-06-22  ...   \n",
      "9687                44.0   Male      1978-12-26  2004-06-22  ...   \n",
      "120224              35.0   Male      1987-01-24         NaN  ...   \n",
      "125788              34.0   Male      1987-01-24         NaN  ...   \n",
      "131471              34.0   Male      1987-01-24         NaN  ...   \n",
      "130122              33.0   Male      1987-01-24         NaN  ...   \n",
      "123192              35.0   Male      1987-01-24         NaN  ...   \n",
      "19664               29.0   Male      1987-01-24         NaN  ...   \n",
      "\n",
      "       Hier_Lev_3_txtA Hier_Lev_4_txtA Hier_Lev_5_txtA ProductCollectionA  \\\n",
      "122626             NaN             NaN             NaN                NaN   \n",
      "134321             NaN             NaN             NaN                NaN   \n",
      "9874              96.0           337.0           919.0               97.0   \n",
      "125277             NaN             NaN             NaN                NaN   \n",
      "22358             96.0           336.0           722.0               83.0   \n",
      "4845              96.0           337.0           919.0               97.0   \n",
      "110727            46.0             5.0           488.0               51.0   \n",
      "109418             5.0           157.0           539.0               57.0   \n",
      "13262              5.0           157.0           405.0               48.0   \n",
      "117890            72.0           291.0           787.0               95.0   \n",
      "16553             98.0           240.0           193.0                9.0   \n",
      "18388             56.0           234.0           464.0               69.0   \n",
      "409               66.0           282.0           766.0               78.0   \n",
      "20910             90.0           238.0           602.0               72.0   \n",
      "7646             105.0           192.0           823.0               78.0   \n",
      "4039              72.0           291.0           786.0               95.0   \n",
      "20155              5.0           157.0           539.0               57.0   \n",
      "123825            66.0           282.0           766.0               78.0   \n",
      "60779            105.0           192.0            88.0               78.0   \n",
      "113561            96.0           337.0           822.0               78.0   \n",
      "112295             5.0           157.0           539.0               57.0   \n",
      "129793             5.0           157.0           539.0               57.0   \n",
      "444964             5.0           157.0           405.0               48.0   \n",
      "9687               5.0           157.0           405.0               48.0   \n",
      "120224            31.0           128.0           701.0               83.0   \n",
      "125788             5.0            43.0            17.0                1.0   \n",
      "131471             5.0           157.0           405.0               48.0   \n",
      "130122            96.0           337.0            61.0                6.0   \n",
      "123192            43.0           165.0            59.0                6.0   \n",
      "19664             72.0           287.0           775.0               73.0   \n",
      "\n",
      "       in_salesdataset NationalityA Calls  Clienteling  Livechat  Wishlist  \n",
      "122626               0         90.0     0            0         0         0  \n",
      "134321               0         90.0     0            0         0         0  \n",
      "9874                 0         90.0     0            0         0         0  \n",
      "125277               0         90.0     0            0         0         0  \n",
      "22358                1          NaN     0            0         0         0  \n",
      "4845                 0         90.0     0            0         0         0  \n",
      "110727               0        155.0     0            1         0         0  \n",
      "109418               0        155.0     0            1         0         0  \n",
      "13262                0        155.0     0            1         0         0  \n",
      "117890               0        155.0     0            1         0         0  \n",
      "16553                0        155.0     0            1         0         0  \n",
      "18388                0        155.0     0            1         0         0  \n",
      "409                  0        155.0     0            1         0         0  \n",
      "20910                0        155.0     0            1         0         0  \n",
      "7646                 0        155.0     0            1         0         0  \n",
      "4039                 0        155.0     0            1         0         0  \n",
      "20155                0        155.0     0            1         0         0  \n",
      "123825               0        155.0     0            1         0         0  \n",
      "60779                1          NaN     0            1         0         0  \n",
      "113561               0        155.0     0            1         0         0  \n",
      "112295               0        155.0     0            1         0         0  \n",
      "129793               0        155.0     0            1         0         0  \n",
      "444964               1          NaN     0            1         0         0  \n",
      "9687                 0        155.0     0            1         0         0  \n",
      "120224               0        182.0     0            1         0         0  \n",
      "125788               0        182.0     0            1         0         0  \n",
      "131471               0        182.0     0            1         0         0  \n",
      "130122               0        182.0     0            1         0         0  \n",
      "123192               0        182.0     0            1         0         0  \n",
      "19664                0        182.0     0            1         0         0  \n",
      "\n",
      "[30 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_AllSales.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate a table which shows what % of total # of ClientID are present in Calls, Livechat, Clienteling and all 3 of them. \n",
    "## Not so useful... As we have this information through excel already, but it can be a good coding exercise :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ## We could filter [AllSales] in order to obtain only the clientID which repurchased after 4 years. This could allow us to look into details what's special about those clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code \n",
    "#1. Create a new dataframe that is sorted by nb_days_since_last_sale, descending\n",
    "#df_sortedallsalesbydays = df_AllSales.sort_values(\"nb_days_since_last_sale\" , axis=0, ascending=False)\n",
    "\n",
    "#2. Visusalize the top 2 records \n",
    "#print(sortedallsalesbydays[[\"ClientID\", \"nb_days_since_last_sale\"]].head(2))\n",
    "\n",
    "#3. Here we count the number of clients with last sale since more than 1460 days (about 4 years)\n",
    "\n",
    "#df_sortedallsalesbydays[df_sortedallsalesbydays.nb_days_since_last_sale > 1460].filter([\"ClientID\", \"nb_days_since_last_sale\"]).count(axis=0)\n",
    "\n",
    "# Answer: There are 126048 record in our dataset with their last purchase made 4 years ago or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filter [AllSales] by Interaction = 1\n",
    "\n",
    "## What kind of question can we answer with this data? (Brainstorm needed)\n",
    "### Were the purchases made right after communication? \n",
    "#### Compare date of purchase with date of latest interaction\n",
    "### Which type of communication caused the most repurchases ? \n",
    "#### Can we use turnover to evaluate which type of communication made the most revenues?\n",
    "\n",
    " \n",
    "## Clienteling:\n",
    "### What kind of activity category is most effective\n",
    "### Make a ranking by activity category and type\n",
    "### Any link between activity status and repurchase / sale amount? \n",
    "\n",
    "## Wishlist:\n",
    "### Do clients with a wishlist repurchase after 4 years? \n",
    "### Any link between last modified date, created date and purchase date?\n",
    "\n",
    "## All Sales\n",
    "### Make a ranking of product categories repurchased after 4 years in order of magnitude\n",
    "### Make another table with a column to identify if product repurchased is in same category, subcategory,  of first product bought\n",
    "### Whats up with wedding dates / anniversary dates and repurchase?\n",
    "\n",
    "## Calls\n",
    "\n",
    "\n",
    "## Livechat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Yann Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset as much as possible \n",
    "# Then run premilimary anaysis / desc statistics\n",
    "# Recode variables \n",
    "# Logistic regression : \n",
    "# y : Client rebought after 4 years or no\n",
    "\n",
    "# y: look at y=1\n",
    "# try to do some clustering according to key variables (product_category?, wedding?, anniversary?, INTERACTION???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Regarding yann's recommendations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When he says \"reduce dataset as much as possible\",\n",
    "# he means we have to make sure that we have unique values for ClientID\n",
    "# Since Clients appear more than once due to the fact that some of them have purchased more than once,\n",
    "# We must find a way to \"summarize\" every client in a way that we end up with only unique ClientIDs in the ClientID column\n",
    "\n",
    "# For example: \n",
    "\n",
    "\n",
    "examplereduce = {\n",
    "    \"ClientID\": [1,1,2,3],\n",
    "    \"number_of_days_since_last_sale\": [10,20,10,10]}\n",
    "\n",
    "df_examplereduce = pd.DataFrame(examplereduce)\n",
    "\n",
    "print(df_examplereduce, \"\\n\\n\", \"As you can see, ClientID 1 appears twice but we want to 'Summarize' it\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can summarize it by using the groupby function from panda\n",
    "\n",
    "df_examplegroupedby = df_examplereduce.groupby(['ClientID']).mean()\n",
    "\n",
    "print(df_examplegroupedby)\n",
    "\n",
    "# Here we specify that we want to group ClientID by their mean\n",
    "\n",
    "print(\" \\n As you can see, we now only have one record for each ClientID, grouped by the mean of number_of_days_since_last_sale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is easy and pretty straightforward because only the \"number_of_days_since_last_sale\" was summarized\n",
    "## We have to figure out how we want each column to be summarized for each ClientID \n",
    "## Amber and Riccardo: If you could look at each variables from the All_sales dataset and figure out if\n",
    "### 1. We can actually group the variable (for example: some variables can't be grouped, or it is difficult to group them)\n",
    "###   Categorical variables are tricky: (If we look at product_category: \n",
    "###   how do you summarize if someone bought a watch and a necklage?, you cant do average on a categorical variable\n",
    "### However, you can group by the mode, meaning the product_category that appeared the most\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all our columns\n",
    "\n",
    "df_AllSales.head()\n",
    "\n",
    "# There is a default limit of columns that panda will display, If you wish to be able to see all of them: \n",
    "#pd.set_option('max_columns', None)\n",
    "# then df_AllSales.head() again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yann also said to recode the variables\n",
    "# That's something you guys could look into\n",
    "\n",
    "# For example: Look at Product Category & Subproduct:\n",
    "\n",
    "print(df_AllSales.ProductCategory.unique(), \"\\n\\n\", df_AllSales.ProductSubCategory.unique())\n",
    "\n",
    "# We should assign numerical values to them if we determine there is some sort of ranking between them\n",
    "# Otherwise, we have to create a new column for each of them with the value 1 or 0 \n",
    "# We need this to run the logistic regression\n",
    "# Find which one of them comes up the more often and then decide if we want to create a new variable called\n",
    "# \"WATCHES\" which would be [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Where we're heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"Cartier Data Science expert would like to make a clarification on the transactions in the dataset: in the dataset there are not only repurchasing clients coming back after more than 4 years, But also clients repurchasing clients coming back before this threshold. \n",
    "\n",
    "In terms of the project requirement, this does not impact what is expected from you. The statistics about repurchasing customers after 4 years is something that Cartier started to investigate and which would be the best starting point for your research. Of course, if it is possible to also investigate other types of returning customers, it could add a different perspective (and possibly a better insight) of repurchasing customers’ behaviour.\"\"\n",
    "\n",
    "\n",
    "## In the sales It is very useful that we also have clients who DIDNT repurchase after 4 years because we can now run a Logistic Regression which will be y = x1 + x2 + x3 + u, where y = [0,1] \n",
    "## y=0 would indicate Client didn't repurchase after 4 years \n",
    "## y=1 would indicate Client did repurchase after 4 years\n",
    "## Our covariates x1, x2, ..., xi could be things like \"Turnover\", \"Product_Category\", \"Interaction\" -> (those are variables we have to create using the communication datasets we have)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What kind of variables can we construct out of our dataset to include in the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Difference between wedding date and date of purchase\n",
    "## Create interaction variables\n",
    "## Plot collections \\ prod category with repurchase \n",
    "## Plot most of our variables before we include them in our regression \n",
    "## Create dummy variables allowing us to reduce dataset\n",
    "## Create a number of products purchased by ClientID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "pandas-profiling",
     "source": "PIP",
     "version": "3.2.0"
    }
   ],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
