{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "hcXFtCPkDqoxd6Hwrjx1ME",
     "type": "MD"
    }
   },
   "source": [
    "# **This is the collaborative code for the group project QTEM Data Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "RBcxvzmNL2UKbfeFkW7aV7",
     "type": "MD"
    }
   },
   "source": [
    "###  Before editing or contributing to this code, you should create a side notebook where you can test your ideas\n",
    "You can do so by cloning this notebook under another name\n",
    "\n",
    "Also, when adding a cell, make sure you comment and put a title on what you are doing so we can keep track of all steps\n",
    "\n",
    "Have FUN!!!\n",
    "\n",
    "## Markdowns: \n",
    "\n",
    "### You can change the cell type to Markdown in the dropdown list in the top menu\n",
    "### Add a cell, then select Mardown\n",
    "### You can then write text under different format\n",
    "### Use the \"#\" symbol before your text to make it bigger\n",
    "# \"#\" will make it Super large\n",
    "## \"##\" a bit smaller\n",
    "### \"###\" a bit smaller\n",
    "##### \"####\" you get the point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____ __________________ _________ _______ __________ ______________ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Hy5AE4C3yU1sHWUkVI4sTk",
     "type": "MD"
    }
   },
   "source": [
    "## 1. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "l0VMpiYvVHyQGAl5HB81ey",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "vxjFia3zrHzPmN8t1uzcnR",
     "type": "MD"
    }
   },
   "source": [
    "## 2. Importing data and defining datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important information / clarification about the datasets: \n",
    "\n",
    "SENT BY MAIO VITTORIO THROUGH FORUM: \n",
    "\n",
    "\"\"Cartier Data Science expert would like to make a clarification on the transactions in the dataset: in the dataset there are not only repurchasing clients coming back after more than 4 years, But also clients repurchasing clients coming back before this threshold.\n",
    "\n",
    "In terms of the project requirement, this does not impact what is expected from you. The statistics about repurchasing customers after 4 years is something that Cartier started to investigate and which would be the best starting point for your research. Of course, if it is possible to also investigate other types of returning customers, it could add a different perspective (and possibly a better insight) of repurchasing customers’ behaviour.\"\" \n",
    "\n",
    "#### This means we could / should also analyze the data from customers who return after less than 4 years\n",
    "#### I think we should start by the ones coming after 4 years since this is the central point \n",
    "#### We could dig deeper if we deem it necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "nyn7xcYNXoOQngwGrnjdPo",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_Calls = pd.read_csv(\"b. CARTIER_CALLS.csv\")\n",
    "df_Clienteling = pd.read_csv(\"c. CARTIER_CLIENTELING.csv\")\n",
    "df_Livechat = pd.read_csv(\"d. CARTIER_LIVECHAT.csv\")\n",
    "df_PrevSales = pd.read_csv(\"e. CARTIER_PREVIOUS_SALES.csv\")\n",
    "df_Sales = pd.read_csv(\"f. CARTIER_SALES.csv\")\n",
    "df_Wishlist = pd.read_csv(\"g. CARTIER_WISHLIST.csv\")\n",
    "\n",
    "# Merged sales and previous sales dataset with extra column [in_salesdataset] to indicate to which original dataset \n",
    "# the data belong\n",
    "\n",
    "df_Sales['in_salesdataset']=1\n",
    "df_PrevSales['in_salesdataset']=0\n",
    "\n",
    "# Rename column from Sale for the concat of datasets\n",
    "\n",
    "df_Sales.rename(columns = {'articleA':'ArticleA'},inplace = True)\n",
    "\n",
    "df_AllSales = pd.concat([df_Sales, df_PrevSales])\n",
    "df_AllSales = df_AllSales.sort_values(by='ClientID', ascending= True)\n",
    "\n",
    "#to export a dataframe to excel (for siqi): \n",
    "\n",
    "#df_BLABLABLA.to_csv('df_BLABLABLA.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  InvoiceHeader   Channel TransactionDate  \\\n",
      "535924  822099000018770-GEMINI-/BIC/AZRIRBAJP00  Boutique      2020-03-27   \n",
      "207854  306704220714005-GEMINI-/BIC/AZRIRBARP00  Boutique      2022-07-14   \n",
      "\n",
      "        TransactionDate_FYYYY TransactionCategory            ClientID  \\\n",
      "535924                   2020                Sale  0011i00000Vn27ZAAR   \n",
      "207854                   2023                Sale  0011i00000VwfAVAAZ   \n",
      "\n",
      "        AgeAtTransaction  Gender PersonBirthDate WeddingDate  ...  \\\n",
      "535924              84.0  Female      1936-10-15         NaN  ...   \n",
      "207854              53.0  Female      1970-02-04  1997-09-27  ...   \n",
      "\n",
      "       BoutiqueNameA ResidencyCountryA nationalityA  ArticleA Hier_Lev_3_txtA  \\\n",
      "535924          69.0              56.0          NaN  527581.0            61.0   \n",
      "207854         119.0             174.0         58.0     369.0            84.0   \n",
      "\n",
      "       Hier_Lev_4_txtA Hier_Lev_5_txtA  ProductCollectionA  in_salesdataset  \\\n",
      "535924            49.0           278.0                24.0                0   \n",
      "207854             NaN             NaN                78.0                1   \n",
      "\n",
      "        NationalityA  \n",
      "535924          99.0  \n",
      "207854           NaN  \n",
      "\n",
      "[2 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define a sample dataset to work with for speed purposes if necessary (3% of size)\n",
    "\n",
    "#df_AllSalesSample = df_AllSales.head(int(round((len(df_allsales)/(100)),0)))\n",
    "# How to get a random sample of the dataset ??\n",
    "df_AllSalesSample=df_AllSales.sample(frac=0.03)\n",
    "print(df_AllSalesSample.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InvoiceHeader',\n",
       " 'Channel',\n",
       " 'TransactionDate',\n",
       " 'TransactionDate_FYYYY',\n",
       " 'TransactionCategory',\n",
       " 'ClientID',\n",
       " 'AgeAtTransaction',\n",
       " 'Gender',\n",
       " 'PersonBirthDate',\n",
       " 'WeddingDate',\n",
       " 'SpokenLanguage',\n",
       " 'WrittenLanguage',\n",
       " 'FirstSalesDate',\n",
       " 'FirstTransactionDate',\n",
       " 'ProductCategory',\n",
       " 'ProductSubCategory',\n",
       " 'ProductFunction',\n",
       " 'Turnover',\n",
       " 'quantity',\n",
       " 'seq_sales_trs',\n",
       " 'nb_days_since_last_sale',\n",
       " 'PurchasedMarketA',\n",
       " 'PurchasedRegionA',\n",
       " 'ResidencyRegionA',\n",
       " 'ResidencyMarketA',\n",
       " 'BoutiqueNameA',\n",
       " 'ResidencyCountryA',\n",
       " 'nationalityA',\n",
       " 'ArticleA',\n",
       " 'Hier_Lev_3_txtA',\n",
       " 'Hier_Lev_4_txtA',\n",
       " 'Hier_Lev_5_txtA',\n",
       " 'ProductCollectionA',\n",
       " 'in_salesdataset']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_AllSales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2173083"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df_AllSalesWithDuplicates = df_AllSales\n",
    "total_rows_before = df_AllSalesWithDuplicates.shape[0]\n",
    "total_rows_before\n",
    "\n",
    "# We have 2 173 083 rows in All_Sales dataset with duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3837"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "df_AllSales.drop_duplicates(inplace=True)\n",
    "\n",
    "total_rows_after = df_AllSales.shape[0]\n",
    "duplicates_count = total_rows_before - total_rows_after\n",
    "duplicates_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6a7ef9ce3db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Drop irrelevant columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_AllSales\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Hier_Lev_3_txtA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Hier_Lev_4_txtA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Hier_Lev_5_txtA\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4162\u001b[0m         \"\"\"\n\u001b[1;32m-> 4163\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3885\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3887\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m                 \u001b[1;31m# Check if label doesn't exist along axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36misin\u001b[1;34m(self, values, level)\u001b[0m\n\u001b[0;32m   4917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4918\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_index_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4919\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4921\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_string_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_lhs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_rhs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36misin\u001b[1;34m(comps, values)\u001b[0m\n\u001b[0;32m    441\u001b[0m         \u001b[1;31m# If the the values include nan we need to check for nan explicitly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;31m# since np.nan it not equal to np.nan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "\n",
    "df_AllSales.drop([\"Hier_Lev_3_txtA\", \"Hier_Lev_4_txtA\", \"Hier_Lev_5_txtA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "\n",
    "df_AllSales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dUPRadFx3IO8tnqSVXv2MQ",
     "type": "MD"
    }
   },
   "source": [
    "## 3. Quick look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "I6Ly9SvBZdU7pyUvUZWWh9",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# To see the columns we have \n",
    "\n",
    "# Run this line if you wish to be able to visualize all the columns whenever you print out the dataframe\n",
    "\n",
    "#pd.set_option('max_columns', None)\n",
    "\n",
    "print(list(df_AllSales))\n",
    "#print(list(anything else you want to see))\n",
    "\n",
    "df_AllSales.describe()\n",
    "df_AllSales.info()\n",
    "AllSales.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "MRlVkGJQ5qogO55sBV5NCR",
     "type": "MD"
    }
   },
   "source": [
    "## 4. Generating the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "7EUPdfz7NnQkQo0TdeDHZ8",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "### We can generate a profile to obtain description of our datasets\n",
    "### This can take some time if running report on large datasets\n",
    "### We can run the report on our sample dataset\n",
    "\n",
    "# Profile for Sales Sample\n",
    "\n",
    "#Profile_AllSalesSample = ProfileReport(df_Allsales, title=\"Pandas Profiling Report Sales\")\n",
    "\n",
    "# Visualizing the report\n",
    "\n",
    "#Report to iframe (will open report in notebook)\n",
    "\n",
    "#Profile_AllSalesSample.to_notebook_iframe()\n",
    "\n",
    "## Report to HTML (Will open up a new tab)\n",
    "#Profile_AllSalesSample.to_file(output_file='All Sales Sample report')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next we would like to create new columns in [Allsales] to identify wether clientID is present in [Calls], [LiveChat], [Clienteling] or any one of the 3 (we can call this column: \"interaction' and assign 1 if there was interaction of any kind and 0 otherwise.\n",
    "\n",
    "### This will be needed to filter our data and run a multidimentional analysis\n",
    "#### To answer questions regarding contacts and communication between Cartier and Clients, we only need data concerning ClientsID who've had interactions with Cartier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are \n",
      "\n",
      "    ClientID  blabla\n",
      "0         1       2\n",
      "1         2       3\n",
      "2         3       4 \n",
      "    ClientID  dateofcall\n",
      "0         2           2\n",
      "1         3           3\n",
      "2         4           4 \n",
      "    ClientID  blabla  Interaction?\n",
      "0         1       2             0\n",
      "1         2       3             1\n",
      "2         3       4             1\n",
      "   ClientID  blabla  yuh\n",
      "0         1       2    0\n",
      "1         2       3    1\n",
      "2         3       4    1\n"
     ]
    }
   ],
   "source": [
    "# #Code:\n",
    "\n",
    "# Example of what we want\n",
    "\n",
    "ExampleSales1 = {'ClientID': [1,2,3], 'blabla': [2,3,4]}\n",
    "df_ExampleSales1 = pd.DataFrame(ExampleSales1)\n",
    "\n",
    "ExampleCall1= {'ClientID':[2,3,4], 'dateofcall':[2,3,4]}\n",
    "df_ExampleCall1 = pd.DataFrame(ExampleCall1)\n",
    "df_finaldataset = pd.DataFrame()\n",
    "df_finaldataset = df_finaldataset.append(df_ExampleSales1)\n",
    "df_finaldataset['Interaction?'] = [0,1,1]\n",
    "\n",
    "# Result: \n",
    "print(\"Results are \\n\\n\", df_ExampleSales1,\"\\n\", df_ExampleCall1, \"\\n\", df_finaldataset)\n",
    "\n",
    "# if 'Ankit' in df.values :\n",
    "#     print(\"\\nThis value exists in Dataframe\")\n",
    "    \n",
    "#     df['Good'] = np.where(df['points']>20, 'yes', 'no')\n",
    "\n",
    "# df_ExampleSales1['YAh']= np.where(df_ExampleSales1['ClientID'] in df_ExampleCall1.ClientID, '1', '0' )\n",
    "# print(df_ExampleSales1)\n",
    "\n",
    "df_ExampleSales1['yuh'] = 0\n",
    "\n",
    "for i in df_ExampleSales1.ClientID:\n",
    "    if i in df_ExampleCall1.ClientID:\n",
    "        df_ExampleSales1.yuh[i] = 1\n",
    "    else:\n",
    "        df_ExampleSales1.yuh[i] = 0\n",
    "print(df_ExampleSales1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating the new columns 'Calls' 'Clienteling' 'Livechat' 'Wishlist' as zero vectors\n",
    "\n",
    "# df_AllSales['Calls'] = 0\n",
    "# df_AllSales['Clienteling'] = 0\n",
    "# df_AllSales['Livechat'] = 0\n",
    "# df_AllSales['Wishlist'] = 0\n",
    "\n",
    "#Lets use our sample dataset to make it faster\n",
    "\n",
    "df_AllSalesSample['Calls'] = 0\n",
    "df_AllSalesSample['Clienteling'] = 0\n",
    "df_AllSalesSample['Livechat'] = 0\n",
    "df_AllSalesSample['Wishlist'] = 0\n",
    "\n",
    "#Defining =1 when ClientID is present in the corresponding datasets\n",
    "\n",
    "for i in df_AllSalesSample.ClientID:\n",
    "    if i in df_Clienteling.clientID: # clientID not written the same way in Clienteling dataset (we will rename)\n",
    "        df_AllSalesSample.Clienteling[i] = 1\n",
    "    else:\n",
    "        df_AllSalesSample.ClientID[i] = 0\n",
    "        \n",
    "df_AllSalesSample[df_AllSalesSample.Clienteling == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate a table which shows what % of total # of ClientID are present in Calls, Livechat, Clienteling and all 3 of them. \n",
    "## Not so useful... As we have this information through excel already, but it can be a good coding exercise :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. ## We could filter [AllSales] in order to obtain only the clientID which repurchased after 4 years. This could allow us to look into details what's special about those clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code \n",
    "#1. Create a new dataframe that is sorted by nb_days_since_last_sale, descending\n",
    "#df_sortedallsalesbydays = df_AllSales.sort_values(\"nb_days_since_last_sale\" , axis=0, ascending=False)\n",
    "\n",
    "#2. Visusalize the top 2 records \n",
    "#print(sortedallsalesbydays[[\"ClientID\", \"nb_days_since_last_sale\"]].head(2))\n",
    "\n",
    "#3. Here we count the number of clients with last sale since more than 1460 days (about 4 years)\n",
    "\n",
    "#df_sortedallsalesbydays[df_sortedallsalesbydays.nb_days_since_last_sale > 1460].filter([\"ClientID\", \"nb_days_since_last_sale\"]).count(axis=0)\n",
    "\n",
    "# Answer: There are 126048 record in our dataset with their last purchase made 4 years ago or more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Filter [AllSales] by Interaction = 1\n",
    "\n",
    "## What kind of question can we answer with this data? (Brainstorm needed)\n",
    "### Were the purchases made right after communication? \n",
    "#### Compare date of purchase with date of latest interaction\n",
    "### Which type of communication caused the most repurchases ? \n",
    "#### Can we use turnover to evaluate which type of communication made the most revenues?\n",
    "\n",
    " \n",
    "## Clienteling:\n",
    "### What kind of activity category is most effective\n",
    "### Make a ranking by activity category and type\n",
    "### Any link between activity status and repurchase / sale amount? \n",
    "\n",
    "## Wishlist:\n",
    "### Do clients with a wishlist repurchase after 4 years? \n",
    "### Any link between last modified date, created date and purchase date?\n",
    "\n",
    "## All Sales\n",
    "### Make a ranking of product categories repurchased after 4 years in order of magnitude\n",
    "### Make another table with a column to identify if product repurchased is in same category, subcategory,  of first product bought\n",
    "### Whats up with wedding dates / anniversary dates and repurchase?\n",
    "\n",
    "## Calls\n",
    "\n",
    "\n",
    "## Livechat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Yann Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset as much as possible \n",
    "# Then run premilimary anaysis / desc statistics\n",
    "# Recode variables \n",
    "# Logistic regression : \n",
    "# y : Client rebought after 4 years or no\n",
    "\n",
    "# y: look at y=1\n",
    "# try to do some clustering according to key variables (product_category?, wedding?, anniversary?, INTERACTION???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Regarding yann's recommendations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When he says \"reduce dataset as much as possible\",\n",
    "# he means we have to make sure that we have unique values for ClientID\n",
    "# Since Clients appear more than once due to the fact that some of them have purchased more than once,\n",
    "# We must find a way to \"summarize\" every client in a way that we end up with only unique ClientIDs in the ClientID column\n",
    "\n",
    "# For example: \n",
    "\n",
    "\n",
    "examplereduce = {\n",
    "    \"ClientID\": [1,1,2,3],\n",
    "    \"number_of_days_since_last_sale\": [10,20,10,10]}\n",
    "\n",
    "df_examplereduce = pd.DataFrame(examplereduce)\n",
    "\n",
    "print(df_examplereduce, \"\\n\\n\", \"As you can see, ClientID 1 appears twice but we want to 'Summarize' it\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can summarize it by using the groupby function from panda\n",
    "\n",
    "df_examplegroupedby = df_examplereduce.groupby(['ClientID']).mean()\n",
    "\n",
    "print(df_examplegroupedby)\n",
    "\n",
    "# Here we specify that we want to group ClientID by their mean\n",
    "\n",
    "print(\" \\n As you can see, we now only have one record for each ClientID, grouped by the mean of number_of_days_since_last_sale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is easy and pretty straightforward because only the \"number_of_days_since_last_sale\" was summarized\n",
    "## We have to figure out how we want each column to be summarized for each ClientID \n",
    "## Amber and Riccardo: If you could look at each variables from the All_sales dataset and figure out if\n",
    "### 1. We can actually group the variable (for example: some variables can't be grouped, or it is difficult to group them)\n",
    "###   Categorical variables are tricky: (If we look at product_category: \n",
    "###   how do you summarize if someone bought a watch and a necklage?, you cant do average on a categorical variable\n",
    "### However, you can group by the mode, meaning the product_category that appeared the most\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all our columns\n",
    "\n",
    "df_AllSales.head()\n",
    "\n",
    "# There is a default limit of columns that panda will display, If you wish to be able to see all of them: \n",
    "#pd.set_option('max_columns', None)\n",
    "# then df_AllSales.head() again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yann also said to recode the variables\n",
    "# That's something you guys could look into\n",
    "\n",
    "# For example: Look at Product Category & Subproduct:\n",
    "\n",
    "print(df_AllSales.ProductCategory.unique(), \"\\n\\n\", df_AllSales.ProductSubCategory.unique())\n",
    "\n",
    "# We should assign numerical values to them if we determine there is some sort of ranking between them\n",
    "# Otherwise, we have to create a new column for each of them with the value 1 or 0 \n",
    "# We need this to run the logistic regression\n",
    "# Find which one of them comes up the more often and then decide if we want to create a new variable called\n",
    "# \"WATCHES\" which would be [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Where we're heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"Cartier Data Science expert would like to make a clarification on the transactions in the dataset: in the dataset there are not only repurchasing clients coming back after more than 4 years, But also clients repurchasing clients coming back before this threshold. \n",
    "\n",
    "In terms of the project requirement, this does not impact what is expected from you. The statistics about repurchasing customers after 4 years is something that Cartier started to investigate and which would be the best starting point for your research. Of course, if it is possible to also investigate other types of returning customers, it could add a different perspective (and possibly a better insight) of repurchasing customers’ behaviour.\"\"\n",
    "\n",
    "\n",
    "## In the sales It is very useful that we also have clients who DIDNT repurchase after 4 years because we can now run a Logistic Regression which will be y = x1 + x2 + x3 + u, where y = [0,1] \n",
    "## y=0 would indicate Client didn't repurchase after 4 years \n",
    "## y=1 would indicate Client did repurchase after 4 years\n",
    "## Our covariates x1, x2, ..., xi could be things like \"Turnover\", \"Product_Category\", \"Interaction\" -> (those are variables we have to create using the communication datasets we have)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What kind of variables can we construct out of our dataset to include in the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Difference between wedding date and date of purchase\n",
    "## Create interaction variables\n",
    "## Plot collections \\ prod category with repurchase \n",
    "## Plot most of our variables before we include them in our regression \n",
    "## Create dummy variables allowing us to reduce dataset\n",
    "## Create a number of products purchased by ClientID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "pandas-profiling",
     "source": "PIP",
     "version": "3.2.0"
    }
   ],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
